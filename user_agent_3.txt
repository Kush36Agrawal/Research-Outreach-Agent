
#############################################################################################################
South Africa
#############################################################################################################


Name: Willem Visser

Abstract for Research Paper 1

Shifting Left for Early Detection of Machine-Learning Bugs | SpringerLink
Your privacy, your choice
We use essential cookies to make sure the site can function. We also use optional cookies for advertising, personalisation of content, usage analysis, and social media.By accepting optional cookies, you consent to the processing of your personal data - including transfers to third parties. Some third parties are outside of the European Economic Area, with varying standards of data protection.See our privacy policy for more information on the use of your personal data.Manage preferences for further information and to change your choices.
Accept all cookies
Skip to main content
Advertisement
Search
Search by keyword or author
Search
Navigation
Find a journal
Publish with us
Track your research
Shifting Left for Early Detection of Machine-Learning Bugs
Conference paper
First Online: 03 March 2023
pp 584–597
Cite this conference paper
Formal Methods
(FM 2023)
AbstractComputational notebooks are widely used for machine learning (ML). However, notebooks raise new correctness concerns beyond those found in traditional programming environments. ML library APIs are easy to misuse, and the notebook execution model raises entirely new problems concerning reproducibility. It is common to use static analyses to detect bugs and enforce best practices in software applications. However, when configured with new types of rules tailored to notebooks, these analyses can also detect notebook-specific problems. We present our initial efforts in understanding how static analysis for notebooks differs from analysis of traditional application software. We created six new rules for the CodeGuru Reviewer based on discussions with ML practitioners. We ran the tool on close to 10,000 experimentation notebooks, resulting in an average of approximately 1 finding per 7 notebooks. Approximately 60% of the findings that we reviewed are real notebook defects. (Due to confidentiality limitations, we cannot disclose the exact number of notebook files and findings.)
This is a preview of subscription content, log in via an institution
to check access.
Access this chapter
Log in via an institution
Subscribe and save
Springer+ Basic
€32.70 /Month
Get 10 units per month
Download Article/Chapter or eBook
1 Unit = 1 Article or 1 Chapter
Cancel anytime
Subscribe now
Buy Now
Chapter
EUR 29.95
Price includes VAT (India)
Available as PDF
Read on any device
Instant download
Own it forever
Buy Chapter
eBook
EUR 85.59
Price includes VAT (India)
Available as EPUB and PDF
Read on any device
Instant download
Own it forever
Buy eBook
Softcover Book
EUR 99.99
Price excludes VAT (India)
Compact, lightweight edition
Dispatched in 3 to 5 business days
Free shipping worldwide - see info
Buy Softcover Book
Tax calculation will be finalised at checkout
Purchases are for personal use only
Institutional subscriptions
Similar content being viewed by others
Can static analysis tools find more defects?
Article
08 November 2022
Supporting Code Review by Automatic Detection of Potentially Buggy Changes
Chapter
© 2015
Static Testing
Chapter
© 2019
ReferencesAbadi, M., et al.: TensorFlow: large-scale machine learning on heterogeneous systems (2015). https://www.tensorflow.org/Apache: Apache MXNet (2022). https://mxnet.apache.org/versions/1.9.1/Bessey, A., et al.: A few billion lines of code later: using static analysis to find bugs in the real world. Commun. ACM 53(2), 66–75 (2010). https://doi.org/10.1145/1646353.1646374. ISSN 0001-0782Article
Google Scholar
Boyd, S., Boyd, S.P., Vandenberghe, L.: Convex Optimization. Cambridge University Press, Cambridge (2004)Book
MATH
Google Scholar
Chollet, F., et al.: Keras (2015). https://keras.ioDilhara, M., Ketkar, A., Dig, D.: Understanding software-2.0: a study of machine learning library usage and evolution. ACM Trans. Softw. Eng. Methodol. 30(4) (2021). https://doi.org/10.1145/3453478. ISSN 1049-331XDistefano, D., Fähndrich, M., Logozzo, F., O’Hearn, P.W.: Scaling static analyses at Facebook. Commun. ACM 62(8), 62–70 (2019). https://doi.org/10.1145/3338112. ISSN 0001-0782Article
Google Scholar
Dolby, J., Shinnar, A., Allain, A., Reinen, J.: Ariadne: analysis for machine learning programs. In: Proceedings of the 2nd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages, MAPL 2018, pp. 1–10. Association for Computing Machinery, New York (2018). https://doi.org/10.1145/3211346.3211349. ISBN 9781450358347Gamma, E., Helm, R., Johnson, R., Vlissides, J.: Design patterns: abstraction and reuse of object-oriented design. In: Nierstrasz, O.M. (ed.) ECOOP 1993. LNCS, vol. 707, pp. 406–431. Springer, Heidelberg (1993). https://doi.org/10.1007/3-540-47910-4_21Chapter
Google Scholar
Gehr, T., Mirman, M., Drachsler-Cohen, D., Tsankov, P., Chaudhuri, S., Vechev, M.: AI2: safety and robustness certification of neural networks with abstract interpretation. In: 2018 IEEE Symposium on Security and Privacy (SP), pp. 3–18 (2018). https://doi.org/10.1109/SP.2018.00058Grotov, K., Titov, S., Sotnikov, V., Golubev, Y., Bryksin, T.: A large-scale comparison of Python code in Jupyter notebooks and scripts. In: Proceedings of the 19th International Conference on Mining Software Repositories, MSR 2022, pp. 353–364. Association for Computing Machinery, New York (2022). https://doi.org/10.1145/3524842.3528447. ISBN 9781450393034Guest, G., Bunce, A., Johnson, L.: How many interviews are enough? An experiment with data saturation and variability. Field Methods 18(1), 59–82 (2006)Article
Google Scholar
Humbatova, N., Jahangirova, G., Bavota, G., Riccio, V., Stocco, A., Tonella, P.: Taxonomy of real faults in deep learning systems. In: Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering, ICSE 2020, pp. 1110–1121. Association for Computing Machinery, New York (2020). https://doi.org/10.1145/3377811.3380395. ISBN 9781450371216Ioffe, S., Szegedy, C.: Batch normalization: accelerating deep network training by reducing internal covariate shift. In: International Conference on Machine Learning, pp. 448–456. PMLR (2015)
Google Scholar
Kluyver, T., et al.: Jupyter notebooks - a publishing format for reproducible computational workflows. In: Loizides, F., Scmidt, B. (eds.) Positioning and Power in Academic Publishing: Players, Agents and Agendas, pp. 87–90. IOS Press (2016). https://eprints.soton.ac.uk/403913/Lagouvardos, S., Dolby, J., Grech, N., Antoniadis, A., Smaragdakis, Y.: Static analysis of shape in TensorFlow programs. In: Hirschfeld, R., Pape, T. (eds.) 34th European Conference on Object-Oriented Programming (ECOOP 2020). Leibniz International Proceedings in Informatics (LIPIcs), vol. 166, pp. 15:1–15:29. Schloss Dagstuhl-Leibniz-Zentrum für Informatik, Dagstuhl (2020). https://doi.org/10.4230/LIPIcs.ECOOP.2020.15, https://drops.dagstuhl.de/opus/volltexte/2020/13172. ISBN 978-3-95977-154-2, ISSN 1868-8969LeCun, Y., Touresky, D., Hinton, G., Sejnowski, T.: A theoretical framework for back-propagation. In: Proceedings of the 1988 Connectionist Models Summer School, vol. 1, pp. 21–28 (1988)
Google Scholar
Liu, C., et al.: Detecting TensorFlow program bugs in real-world industrial environment. In: 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE), pp. 55–66 (2021). https://doi.org/10.1109/ASE51524.2021.9678891Madhyastha, P., Jain, R.: On model stability as a function of random seed. arXiv preprint arXiv:1909.10447 (2019)Microsoft: Pyright: Static type checker for Python (2022). https://github.com/microsoft/pyrightMukherjee, R., Tripp, O., Liblit, B., Wilson, M.: Static analysis for AWS best practices in Python code. In: Ali, K., Vitek, J. (eds.) 36th European Conference on Object-Oriented Programming, ECOOP 2022, 6–10 June 2022, Berlin, Germany. LIPIcs, vol. 222, pp. 14:1–14:28. Schloss Dagstuhl - Leibniz-Zentrum für Informatik (2022), https://doi.org/10.4230/LIPIcs.ECOOP.2022.14Paszke, A., et al.: PyTorch: an imperative style, high-performance deep learning library. Adv. Neural Inf. Process. Syst. 32 (2019)
Google Scholar
Pimentel, J.A.F., Murta, L., Braganholo, V., Freire, J.: A large-scale study about quality and reproducibility of Jupyter notebooks. In: Proceedings of the 16th International Conference on Mining Software Repositories, MSR 2019, pp. 507–517. IEEE Press (2019). https://doi.org/10.1109/MSR.2019.00077Python Software Foundation: The Python standard library: typing—support for type hints: typing.Union (2022). https://docs.python.org/3/library/typing.html#typing.UnionPython Software Foundation: The Python standard library: typing—support for type hints: The Any type (2022). https://docs.python.org/3/library/typing.html#the-any-typeQuaranta, L.: Assessing the quality of computational notebooks for a frictionless transition from exploration to production. In: Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: Companion Proceedings, ICSE 2022, pp. 256–260. Association for Computing Machinery, New York (2022). https://doi.org/10.1145/3510454.3517055. ISBN 9781450392235Quaranta, L., Calefato, F., Lanubile, F.: Pynblint: a static analyzer for Python Jupyter notebooks. In: 2022 IEEE/ACM 1st International Conference on AI Engineering - Software Engineering for AI (CAIN), pp. 48–49 (2022). https://doi.org/10.1145/3522664.3528612Rasley, J., Rajbhandari, S., Ruwase, O., He, Y.: DeepSpeed: system optimizations enable training deep learning models with over 100 billion parameters. In: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 3505–3506 (2020)
Google Scholar
Research, I.: WALA: The T. J. Watson libraries for analysis (2022). https://github.com/wala/WALARuder, S.: An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747 (2016)Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.: Dropout: a simple way to prevent neural networks from overfitting. J. Mach. Learn. Res. 15(1), 1929–1958 (2014)MathSciNet
MATH
Google Scholar
Subotić, P., Milikić, L., Stojić, M.: A static analysis framework for data science notebooks. In: Proceedings of the 44th International Conference on Software Engineering: Software Engineering in Practice, ICSE-SEIP 2022, pp. 13–22. Association for Computing Machinery, New York (2022). https://doi.org/10.1145/3510457.3513032. ISBN 9781450392266Urban, C.: Static analysis of data science software. In: Chang, B.-Y.E. (ed.) SAS 2019. LNCS, vol. 11822, pp. 17–23. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-32304-2_2. ISBN 978-3-030-32304-2Wan, C., Liu, S., Hoffmann, H., Maire, M., Lu, S.: Are machine learning cloud APIs used correctly? In: 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE), pp. 125–137 (2021). https://doi.org/10.1109/ICSE43902.2021.00024Wan, Z., Xia, X., Lo, D., Murphy, G.C.: How does machine learning change software development practices? IEEE Trans. Software Eng. 47(9), 1857–1871 (2021). https://doi.org/10.1109/TSE.2019.2937083Article
Google Scholar
Wang, J., Kuo, T.y., Li, L., Zeller, A.: Restoring reproducibility of Jupyter notebooks. In: 2020 IEEE/ACM 42nd International Conference on Software Engineering: Companion Proceedings (ICSE-Companion), pp. 288–289 (2020)
Google Scholar
Wu, D., Shen, B., Chen, Y., Jiang, H., Qiao, L.: Tensfa: detecting and repairing tensor shape faults in deep learning systems. In: 2021 IEEE 32nd International Symposium on Software Reliability Engineering (ISSRE), pp. 11–21 (2021). https://doi.org/10.1109/ISSRE52982.2021.00014Zhang, Y., Ren, L., Chen, L., Xiong, Y., Cheung, S.C., Xie, T.: Detecting numerical bugs in neural network architectures. In: Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/FSE 2020, pp. 826–837. Association for Computing Machinery, New York (2020). https://doi.org/10.1145/3368089.3409720. ISBN 9781450370431Download references Author informationAuthors and AffiliationsAmazon Web Services, Arlington, USABen LiblitAmazon Web Services, Berlin, GermanyLinghui Luo & Goran PiskachevAmazon, Seattle, USAAlejandro MolinaThe University of Texas at Dallas, Richardson, USAZachary PattersonAmazon Web Services, Santa Clara, USARajdeep Mukherjee, Omer Tripp & Willem VisserAmazon Web Services, New York, USAMartin SchäfAuthorsBen LiblitView author publicationsYou can also search for this author in
PubMed Google ScholarLinghui LuoView author publicationsYou can also search for this author in
PubMed Google ScholarAlejandro MolinaView author publicationsYou can also search for this author in
PubMed Google ScholarRajdeep MukherjeeView author publicationsYou can also search for this author in
PubMed Google ScholarZachary PattersonView author publicationsYou can also search for this author in
PubMed Google ScholarGoran PiskachevView author publicationsYou can also search for this author in
PubMed Google ScholarMartin SchäfView author publicationsYou can also search for this author in
PubMed Google ScholarOmer TrippView author publicationsYou can also search for this author in
PubMed Google ScholarWillem VisserView author publicationsYou can also search for this author in
PubMed Google ScholarCorresponding authorCorrespondence to
Linghui Luo . Editor informationEditors and AffiliationsUniversity of Toronto, Toronto, ON, CanadaMarsha Chechik RWTH Aachen University, Aachen, GermanyJoost-Pieter Katoen University of Lübeck, Lübeck, GermanyMartin Leucker
Rights and permissionsReprints and permissions Copyright information© 2023 The Author(s), under exclusive license to Springer Nature Switzerland AG About this paperCite this paperLiblit, B. et al. (2023).
Shifting Left for Early Detection of Machine-Learning Bugs.
In: Chechik, M., Katoen, JP., Leucker, M. (eds) Formal Methods. FM 2023. Lecture Notes in Computer Science, vol 14000. Springer, Cham. https://doi.org/10.1007/978-3-031-27481-7_33Download citation.RIS.ENW.BIBDOI: https://doi.org/10.1007/978-3-031-27481-7_33Published: 03 March 2023
Publisher Name: Springer, Cham
Print ISBN: 978-3-031-27480-0
Online ISBN: 978-3-031-27481-7eBook Packages: Computer ScienceComputer Science (R0)Share this paperAnyone you share the following link with will be able to read this content:Get shareable linkSorry, a shareable link is not currently available for this article.Copy to clipboard
Provided by the Springer Nature SharedIt content-sharing initiative
Publish with usPolicies and ethics
Access this chapter
Log in via an institution
Subscribe and save
Springer+ Basic
€32.70 /Month
Get 10 units per month
Download Article/Chapter or eBook
1 Unit = 1 Article or 1 Chapter
Cancel anytime
Subscribe now
Buy Now
Chapter
EUR 29.95
Price includes VAT (India)
Available as PDF
Read on any device
Instant download
Own it forever
Buy Chapter
eBook
EUR 85.59
Price includes VAT (India)
Available as EPUB and PDF
Read on any device
Instant download
Own it forever
Buy eBook
Softcover Book
EUR 99.99
Price excludes VAT (India)
Compact, lightweight edition
Dispatched in 3 to 5 business days
Free shipping worldwide - see info
Buy Softcover Book
Tax calculation will be finalised at checkout
Purchases are for personal use only
Institutional subscriptions
SectionsReferences Abstract References Author information Editor information Rights and permissions Copyright information About this paper Publish with us
Abadi, M., et al.: TensorFlow: large-scale machine learning on heterogeneous systems (2015). https://www.tensorflow.org/Apache: Apache MXNet (2022). https://mxnet.apache.org/versions/1.9.1/Bessey, A., et al.: A few billion lines of code later: using static analysis to find bugs in the real world. Commun. ACM 53(2), 66–75 (2010). https://doi.org/10.1145/1646353.1646374. ISSN 0001-0782Article
Google Scholar Boyd, S., Boyd, S.P., Vandenberghe, L.: Convex Optimization. Cambridge University Press, Cambridge (2004)Book MATH
Google Scholar Chollet, F., et al.: Keras (2015). https://keras.ioDilhara, M., Ketkar, A., Dig, D.: Understanding software-2.0: a study of machine learning library usage and evolution. ACM Trans. Softw. Eng. Methodol. 30(4) (2021). https://doi.org/10.1145/3453478. ISSN 1049-331XDistefano, D., Fähndrich, M., Logozzo, F., O’Hearn, P.W.: Scaling static analyses at Facebook. Commun. ACM 62(8), 62–70 (2019). https://doi.org/10.1145/3338112. ISSN 0001-0782Article
Google Scholar Dolby, J., Shinnar, A., Allain, A., Reinen, J.: Ariadne: analysis for machine learning programs. In: Proceedings of the 2nd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages, MAPL 2018, pp. 1–10. Association for Computing Machinery, New York (2018). https://doi.org/10.1145/3211346.3211349. ISBN 9781450358347Gamma, E., Helm, R., Johnson, R., Vlissides, J.: Design patterns: abstraction and reuse of object-oriented design. In: Nierstrasz, O.M. (ed.) ECOOP 1993. LNCS, vol. 707, pp. 406–431. Springer, Heidelberg (1993). https://doi.org/10.1007/3-540-47910-4_21Chapter
Google Scholar Gehr, T., Mirman, M., Drachsler-Cohen, D., Tsankov, P., Chaudhuri, S., Vechev, M.: AI2: safety and robustness certification of neural networks with abstract interpretation. In: 2018 IEEE Symposium on Security and Privacy (SP), pp. 3–18 (2018). https://doi.org/10.1109/SP.2018.00058Grotov, K., Titov, S., Sotnikov, V., Golubev, Y., Bryksin, T.: A large-scale comparison of Python code in Jupyter notebooks and scripts. In: Proceedings of the 19th International Conference on Mining Software Repositories, MSR 2022, pp. 353–364. Association for Computing Machinery, New York (2022). https://doi.org/10.1145/3524842.3528447. ISBN 9781450393034Guest, G., Bunce, A., Johnson, L.: How many interviews are enough? An experiment with data saturation and variability. Field Methods 18(1), 59–82 (2006)Article
Google Scholar Humbatova, N., Jahangirova, G., Bavota, G., Riccio, V., Stocco, A., Tonella, P.: Taxonomy of real faults in deep learning systems. In: Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering, ICSE 2020, pp. 1110–1121. Association for Computing Machinery, New York (2020). https://doi.org/10.1145/3377811.3380395. ISBN 9781450371216Ioffe, S., Szegedy, C.: Batch normalization: accelerating deep network training by reducing internal covariate shift. In: International Conference on Machine Learning, pp. 448–456. PMLR (2015)
Google Scholar Kluyver, T., et al.: Jupyter notebooks - a publishing format for reproducible computational workflows. In: Loizides, F., Scmidt, B. (eds.) Positioning and Power in Academic Publishing: Players, Agents and Agendas, pp. 87–90. IOS Press (2016). https://eprints.soton.ac.uk/403913/Lagouvardos, S., Dolby, J., Grech, N., Antoniadis, A., Smaragdakis, Y.: Static analysis of shape in TensorFlow programs. In: Hirschfeld, R., Pape, T. (eds.) 34th European Conference on Object-Oriented Programming (ECOOP 2020). Leibniz International Proceedings in Informatics (LIPIcs), vol. 166, pp. 15:1–15:29. Schloss Dagstuhl-Leibniz-Zentrum für Informatik, Dagstuhl (2020). https://doi.org/10.4230/LIPIcs.ECOOP.2020.15, https://drops.dagstuhl.de/opus/volltexte/2020/13172. ISBN 978-3-95977-154-2, ISSN 1868-8969LeCun, Y., Touresky, D., Hinton, G., Sejnowski, T.: A theoretical framework for back-propagation. In: Proceedings of the 1988 Connectionist Models Summer School, vol. 1, pp. 21–28 (1988)
Google Scholar Liu, C., et al.: Detecting TensorFlow program bugs in real-world industrial environment. In: 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE), pp. 55–66 (2021). https://doi.org/10.1109/ASE51524.2021.9678891Madhyastha, P., Jain, R.: On model stability as a function of random seed. arXiv preprint arXiv:1909.10447 (2019)Microsoft: Pyright: Static type checker for Python (2022). https://github.com/microsoft/pyrightMukherjee, R., Tripp, O., Liblit, B., Wilson, M.: Static analysis for AWS best practices in Python code. In: Ali, K., Vitek, J. (eds.) 36th European Conference on Object-Oriented Programming, ECOOP 2022, 6–10 June 2022, Berlin, Germany. LIPIcs, vol. 222, pp. 14:1–14:28. Schloss Dagstuhl - Leibniz-Zentrum für Informatik (2022), https://doi.org/10.4230/LIPIcs.ECOOP.2022.14Paszke, A., et al.: PyTorch: an imperative style, high-performance deep learning library. Adv. Neural Inf. Process. Syst. 32 (2019)
Google Scholar Pimentel, J.A.F., Murta, L., Braganholo, V., Freire, J.: A large-scale study about quality and reproducibility of Jupyter notebooks. In: Proceedings of the 16th International Conference on Mining Software Repositories, MSR 2019, pp. 507–517. IEEE Press (2019). https://doi.org/10.1109/MSR.2019.00077Python Software Foundation: The Python standard library: typing—support for type hints: typing.Union (2022). https://docs.python.org/3/library/typing.html#typing.UnionPython Software Foundation: The Python standard library: typing—support for type hints: The Any type (2022). https://docs.python.org/3/library/typing.html#the-any-typeQuaranta, L.: Assessing the quality of computational notebooks for a frictionless transition from exploration to production. In: Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: Companion Proceedings, ICSE 2022, pp. 256–260. Association for Computing Machinery, New York (2022). https://doi.org/10.1145/3510454.3517055. ISBN 9781450392235Quaranta, L., Calefato, F., Lanubile, F.: Pynblint: a static analyzer for Python Jupyter notebooks. In: 2022 IEEE/ACM 1st International Conference on AI Engineering - Software Engineering for AI (CAIN), pp. 48–49 (2022). https://doi.org/10.1145/3522664.3528612Rasley, J., Rajbhandari, S., Ruwase, O., He, Y.: DeepSpeed: system optimizations enable training deep learning models with over 100 billion parameters. In: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 3505–3506 (2020)
Google Scholar Research, I.: WALA: The T. J. Watson libraries for analysis (2022). https://github.com/wala/WALARuder, S.: An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747 (2016)Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.: Dropout: a simple way to prevent neural networks from overfitting. J. Mach. Learn. Res. 15(1), 1929–1958 (2014)MathSciNet MATH
Google Scholar Subotić, P., Milikić, L., Stojić, M.: A static analysis framework for data science notebooks. In: Proceedings of the 44th International Conference on Software Engineering: Software Engineering in Practice, ICSE-SEIP 2022, pp. 13–22. Association for Computing Machinery, New York (2022). https://doi.org/10.1145/3510457.3513032. ISBN 9781450392266Urban, C.: Static analysis of data science software. In: Chang, B.-Y.E. (ed.) SAS 2019. LNCS, vol. 11822, pp. 17–23. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-32304-2_2. ISBN 978-3-030-32304-2Wan, C., Liu, S., Hoffmann, H., Maire, M., Lu, S.: Are machine learning cloud APIs used correctly? In: 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE), pp. 125–137 (2021). https://doi.org/10.1109/ICSE43902.2021.00024Wan, Z., Xia, X., Lo, D., Murphy, G.C.: How does machine learning change software development practices? IEEE Trans. Software Eng. 47(9), 1857–1871 (2021). https://doi.org/10.1109/TSE.2019.2937083Article
Google Scholar Wang, J., Kuo, T.y., Li, L., Zeller, A.: Restoring reproducibility of Jupyter notebooks. In: 2020 IEEE/ACM 42nd International Conference on Software Engineering: Companion Proceedings (ICSE-Companion), pp. 288–289 (2020)
Google Scholar Wu, D., Shen, B., Chen, Y., Jiang, H., Qiao, L.: Tensfa: detecting and repairing tensor shape faults in deep learning systems. In: 2021 IEEE 32nd International Symposium on Software Reliability Engineering (ISSRE), pp. 11–21 (2021). https://doi.org/10.1109/ISSRE52982.2021.00014Zhang, Y., Ren, L., Chen, L., Xiong, Y., Cheung, S.C., Xie, T.: Detecting numerical bugs in neural network architectures. In: Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/FSE 2020, pp. 826–837. Association for Computing Machinery, New York (2020). https://doi.org/10.1145/3368089.3409720. ISBN 9781450370431

Abstract for Research Paper 2

Neural-Based Test Oracle Generation: A Large-Scale Evaluation and Lessons Learned | Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering
This website uses cookiesWe occasionally run membership recruitment campaigns on social media channels and use cookies to track post-clicks. We also share information about your use of our site with our social media, advertising and analytics partners who may combine it with other information that you’ve provided to them or that they’ve collected from your use of their services. Use the check boxes below to choose the types of cookies you consent to have stored on your device. Do not sell or share my personal informationUse necessary cookies only Allow all cookies Show detailsOKUse necessary cookies only Allow selected cookies Allow all cookies Necessary Preferences Statistics MarketingShow detailsCookie declaration [#IABV2SETTINGS#] About Necessary (10)  Preferences (5)  Statistics (16)  Marketing (24)  Unclassified (10)Necessary cookies help make a website usable by enabling basic functions like page navigation and access to secure areas of the website. The website cannot function properly without these cookies. These cookies do not gather information about you that could be used for marketing purposes and do not remember where you have been on the internet.NameProviderPurposeMaximum Storage DurationType__cf_bm [x2]ACMThis cookie is used to distinguish between humans and bots. This is beneficial for the website, in order to make valid reports on the use of their website.1 dayHTTP Cookie__jidc.disquscdn.comUsed to add comments to the website and remember the user's Disqus login credentials across websites that use said service.SessionHTTP Cookiedisqusauthc.disquscdn.comRegisters whether the user is logged in. This allows the website owner to make parts of the website inaccessible, based on the user's log-in status.
SessionHTTP Cookie_cfuvidACMThis cookie is a part of the services provided by Cloudflare - Including load-balancing, deliverance of website content and serving DNS connection for website operators. SessionHTTP CookieCookieConsentCookiebotStores the user's cookie consent state for the current domain1 yearHTTP CookieJSESSIONIDACMPreserves users states across page requests.SessionHTTP Cookie_gh_sessGithubPreserves users states across page requests.SessionHTTP Cookielogged_inGithubRegisters whether the user is logged in. This allows the website owner to make parts of the website inaccessible, based on the user's log-in status.
1 yearHTTP Cookie1.gifCookiebotUsed to count the number of sessions to the website, necessary for optimizing CMP product delivery. SessionPixel TrackerPreference cookies enable a website to remember information that changes the way the website behaves or looks, like your preferred language or the region that you are in.NameProviderPurposeMaximum Storage DurationTypeaet-dismissc.disquscdn.comNecessary for the functionality of the website's comment-system.PersistentHTML Local Storagedrafts.queuec.disquscdn.comNecessary for the functionality of the website's comment-system.PersistentHTML Local Storagesubmitted_posts_cachec.disquscdn.comNecessary for the functionality of the website's comment-system.PersistentHTML Local StoragemopDeployMopinionPendingSessionHTML Local StorageMACHINE_LAST_SEENACMPending300 daysHTTP CookieStatistic cookies help website owners understand how visitors interact with websites by collecting and reporting information anonymously.NameProviderPurposeMaximum Storage DurationType_gaGoogleRegisters a unique ID that is used to generate statistical data on how the visitor uses the website.2 yearsHTTP Cookie_ga_#GoogleUsed by Google Analytics to collect data on the number of times a user has visited the website as well as dates for the first and most recent visit. 2 yearsHTTP Cookie_gatGoogleUsed by Google Analytics to throttle request rate1 dayHTTP Cookie_gidGoogleRegisters a unique ID that is used to generate statistical data on how the visitor uses the website.1 dayHTTP Cookie_hjSession_#HotjarCollects statistics on the visitor's visits to the website, such as the number of visits, average time spent on the website and what pages have been read.1 dayHTTP Cookie_hjSessionUser_#HotjarCollects statistics on the visitor's visits to the website, such as the number of visits, average time spent on the website and what pages have been read.1 yearHTTP Cookie_hjTLDTestHotjarRegisters statistical data on users' behaviour on the website. Used for internal analytics by the website operator. SessionHTTP Cookie_hp2_#Heap AnalyticsCollects data on the user’s navigation and behavior on the website. This is used to compile statistical reports and heatmaps for the website owner.1 dayHTTP Cookie_hp2_hld#.#Heap AnalyticsCollects data on the user’s navigation and behavior on the website. This is used to compile statistical reports and heatmaps for the website owner.1 dayHTTP Cookie_hp2_id.#Heap AnalyticsCollects data on the user’s navigation and behavior on the website. This is used to compile statistical reports and heatmaps for the website owner.13 monthsHTTP Cookie_hp2_ses_props.#Heap AnalyticsCollects data on the user’s navigation and behavior on the website. This is used to compile statistical reports and heatmaps for the website owner.1 dayHTTP Cookiedisqus_uniquec.disquscdn.comCollects statistics related to the user's visits to the website, such as number of visits, average time spent on the website and loaded pages.SessionHTTP Cookie_octoGithubPending1 yearHTTP CookiecollectGoogleUsed to send data to Google Analytics about the visitor's device and behavior. Tracks the visitor across devices and marketing channels.SessionPixel TrackerhjActiveViewportIdsHotjarThis cookie contains an ID string on the current session. This contains non-personal information on what subpages the visitor enters – this information is used to optimize the visitor's experience.PersistentHTML Local StoragehjViewportIdHotjarSaves the user's screen size in order to adjust the size of images on the website.SessionHTML Local StorageMarketing cookies are used to track visitors across websites. The intention is to display ads that are relevant and engaging for the individual user and thereby more valuable for publishers and third party advertisers.NameProviderPurposeMaximum Storage DurationTypebadges-messagec.disquscdn.comCollects data on the visitor’s use of the comment system on the website, and what blogs/articles the visitor has read. This can be used for marketing purposes. PersistentHTML Local StorageNIDGooglePending6 monthsHTTP Cookieapi/telemetryHeap AnalyticsCollects data on user behaviour and interaction in order to optimize the website and make advertisement on the website more relevant. SessionPixel TrackerhHeap AnalyticsCollects data on user behaviour and interaction in order to optimize the website and make advertisement on the website more relevant. SessionPixel Tracker#-#YouTubeUsed to track user’s interaction with embedded content.SessionHTML Local StorageiU5q-!O9@$YouTubeRegisters a unique ID to keep statistics of what videos from YouTube the user has seen.SessionHTML Local StorageLAST_RESULT_ENTRY_KEYYouTubeUsed to track user’s interaction with embedded content.SessionHTTP CookieLogsDatabaseV2:V#||LogsRequestsStoreYouTubeUsed to track user’s interaction with embedded content.PersistentIndexedDBnextIdYouTubeUsed to track user’s interaction with embedded content.SessionHTTP Cookieremote_sidYouTubeNecessary for the implementation and functionality of YouTube video-content on the website.
SessionHTTP CookierequestsYouTubeUsed to track user’s interaction with embedded content.SessionHTTP CookieTESTCOOKIESENABLEDYouTubeUsed to track user’s interaction with embedded content.1 dayHTTP CookieVISITOR_INFO1_LIVEYouTubePending180 daysHTTP CookieYSCYouTubePendingSessionHTTP Cookieyt.innertube::nextIdYouTubeRegisters a unique ID to keep statistics of what videos from YouTube the user has seen.PersistentHTML Local Storageytidb::LAST_RESULT_ENTRY_KEYYouTubeUsed to track user’s interaction with embedded content.PersistentHTML Local StorageYtIdbMeta#databasesYouTubeUsed to track user’s interaction with embedded content.PersistentIndexedDByt-remote-cast-availableYouTubeStores the user's video player preferences using embedded YouTube videoSessionHTML Local Storageyt-remote-cast-installedYouTubeStores the user's video player preferences using embedded YouTube videoSessionHTML Local Storageyt-remote-connected-devicesYouTubeStores the user's video player preferences using embedded YouTube videoPersistentHTML Local Storageyt-remote-device-idYouTubeStores the user's video player preferences using embedded YouTube videoPersistentHTML Local Storageyt-remote-fast-check-periodYouTubeStores the user's video player preferences using embedded YouTube videoSessionHTML Local Storageyt-remote-session-appYouTubeStores the user's video player preferences using embedded YouTube videoSessionHTML Local Storageyt-remote-session-nameYouTubeStores the user's video player preferences using embedded YouTube videoSessionHTML Local StorageUnclassified cookies are cookies that we are in the process of classifying, together with the providers of individual cookies.NameProviderPurposeMaximum Storage DurationTypedisqus.threadc.disquscdn.comPendingPersistentHTML Local Storage10.1145%2F3597501_pdfACMPendingPersistentHTML Local Storage10.1145%2F3641524_pdfACMPendingPersistentHTML Local Storagearticle_reader_settingsACMPendingPersistentHTML Local Storagebook_reader_settingsACMPendingPersistentHTML Local Storagechapter_reader_settingsACMPendingPersistentHTML Local Storageissue_reader_settingsACMPendingPersistentHTML Local StorageMAIDACMPending300 daysHTTP CookietipKeyACMPendingPersistentHTML Local StorageGSPGooglePending400 daysHTTP Cookie [#IABV2_LABEL_PURPOSES#]  [#IABV2_LABEL_FEATURES#]  [#IABV2_LABEL_PARTNERS#][#IABV2_BODY_PURPOSES#][#IABV2_BODY_FEATURES#][#IABV2_BODY_PARTNERS#]Cookies are small text files that can be used by websites to make a user's experience more efficient. Other than those strictly necessary for the operation of the site,  we need your permission to store any type of cookies on your device. Learn more about ACM, how you can contact us, and how we process personal data in our Privacy Policy. Also please consult our Cookie Notice.You can change or withdraw your consent from the Cookie Declaration on our website at any time by visiting the Cookie Declaration page. If contacting us regarding your consent, please state your consent ID and date from that page.Your consent applies to the following domains: dl.acm.orgCookie declaration last updated on 12/3/24 by Cookiebot
skip to main content
ContentsInformation & ContributorsBibliometrics & CitationsView OptionsReferencesMediaTablesShareAbstractDefining test oracles is crucial and central to test development, but manual construction of oracles is expensive. While recent neural-based automated test oracle generation techniques have shown promise, their real-world effectiveness remains a compelling question requiring further exploration and understanding. This paper investigates the effectiveness of TOGA, a recently developed neural-based method for automatic test oracle generation. TOGA utilizes EvoSuite-generated test inputs and generates both exception and assertion oracles. In a Defects4j study, TOGA outperformed specification, search, and neural-based techniques, detecting 57 bugs, including 30 unique bugs not detected by other methods. To gain a deeper understanding of its applicability in real-world settings, we conducted a series of external, extended, and conceptual replication studies of TOGA.In a large-scale study involving 25 real-world Java systems, 223.5K test cases, and 51K injected faults, we evaluate TOGA’s ability to improve fault-detection effectiveness relative to the state-of-the-practice and the state-of-the-art. We find that TOGA misclassifies the type of oracle needed 24% of the time and that when it classifies correctly around 62% of the time it is not confident enough to generate any assertion oracle. When it does generate an assertion oracle, more than 47% of them are false positives, and the true positive assertions only increase fault detection by 0.3% relative to prior work. These findings expose limitations of the state-of-the-art neural-based oracle generation technique, provide valuable insights for improvement, and offer lessons for evaluating future automated oracle generation methods.Supplementary Material"Deep Learning (DL) is prevalently used in various industries to improve decision-making and automate processes, driven by the ever-evolving DL libraries and compilers. The correctness of DL systems is crucial for trust in DL applications. As such, the recent wave of research has been studying the automated synthesis of test-cases (i.e., DNN models and their inputs) for fuzzing DL systems. However, existing model generators only subsume a limited number of operators, for lacking the ability to pervasively model operator constraints. To address this challenge, we propose DynoFuzz, a fully automated approach for generating valid and diverse DL models composed of hundreds of types of operators. DynoFuzz adopts a three-step process: (i) collecting valid and invalid API traces from various sources; (ii) applying inductive program synthesis over the traces to infer the constraints for constructing valid models; and (iii) performing hybrid model generation by incorporating both symbolic and concrete operators concolically. Our evaluation shows that DynoFuzz improves branch coverage of TensorFlow and PyTorch by 51% and 15% over the state-of-the-art. Within four months, DynoFuzz finds 86 new bugs for PyTorch and TensorFlow, with 62 already fixed or confirmed, and 8 high-priority bugs labeled by PyTorch, constituting 10% of all high-priority bugs of the period. Additionally, open-source developers regard error-inducing models reported by us as “high-quality” and “common in practice”."Download78.08 MBReferences[1]Shaukat Ali, Lionel C Briand, Hadi Hemmati, and Rajwinder Kaur Panesar-Walawege. 2009. A systematic review of the application and empirical investigation of search-based test case generation. IEEE Transactions on Software Engineering, 36, 6 (2009), 742–762. https://doi.org/10.1109/TSE.2009.52Digital LibraryGoogle Scholar[2]M Moein Almasi, Hadi Hemmati, Gordon Fraser, Andrea Arcuri, and Janis Benefelds. 2017. An industrial evaluation of unit test generation: Finding real faults in a financial application. In 2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering in Practice Track (ICSE-SEIP). 263–272. https://doi.org/10.1109/ICSE-SEIP.2017.27Digital LibraryGoogle Scholar[3]J.H. Andrews, L.C. Briand, and Y. Labiche. 2005. Is mutation an appropriate tool for testing experiments? [software testing]. In Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005. 402–411. https://doi.org/10.1109/ICSE.2005.1553583CrossrefGoogle Scholar[4]Earl T. Barr, Mark Harman, Phil McMinn, Muzammil Shahbaz, and Shin Yoo. 2015. The Oracle Problem in Software Testing: A Survey. IEEE Transactions on Software Engineering, 41, 5 (2015), 507–525. https://doi.org/10.1109/TSE.2014.2372785Digital LibraryGoogle Scholar[5]Moritz Beller, Chu-Pan Wong, Johannes Bader, Andrew Scott, Mateusz Machalica, Satish Chandra, and Erik Meijer. 2021. What it would take to use mutation testing in industry—a study at facebook. In 2021 IEEE/ACM 43rd International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP). 268–277. https://doi.org/10.1109/ICSE-SEIP52600.2021.00036Digital LibraryGoogle Scholar[6]Arianna Blasi, Alberto Goffi, Konstantin Kuznetsov, Alessandra Gorla, Michael D. Ernst, Mauro Pezzè, and Sergio Delgado Castellanos. 2018. Translating Code Comments to Procedure Specifications. In Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis (ISSTA 2018). Association for Computing Machinery, New York, NY, USA. 242–253. isbn:9781450356992 https://doi.org/10.1145/3213846.3213872Digital LibraryGoogle Scholar[7]Arianna Blasi, Alessandra Gorla, Michael D Ernst, Mauro Pezzè, and Antonio Carzaniga. 2021. MeMo: Automatically identifying metamorphic relations in Javadoc comments for test automation. Journal of Systems and Software, 181 (2021), 111041. https://doi.org/10.1016/j.jss.2021.111041Digital LibraryGoogle Scholar[8]Marcel Böhme, Van-Thuan Pham, Manh-Dung Nguyen, and Abhik Roychoudhury. 2017. Directed greybox fuzzing. In Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security. 2329–2344. https://doi.org/10.1145/3133956.3134020Digital LibraryGoogle Scholar[9]José Campos, Andrea Arcuri, Gordon Fraser, and Rui Abreu. 2014. Continuous Test Generation: Enhancing Continuous Integration with Automated Test Generation. In Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering (ASE ’14). Association for Computing Machinery, New York, NY, USA. 55–66. isbn:9781450330138 https://doi.org/10.1145/2642937.2643002Digital LibraryGoogle Scholar[10]Chen Chen, Baojiang Cui, Jinxin Ma, Runpu Wu, Jianchao Guo, and Wenqian Liu. 2018. A systematic review of fuzzing techniques. Computers & Security, 75 (2018), 118–137. https://doi.org/10.1016/j.cose.2018.02.002CrossrefGoogle Scholar[11]John Joseph Chilenski and Steven P Miller. 1994. Applicability of modified condition/decision coverage to software testing. Software Engineering Journal, 9, 5 (1994), 193–200. https://doi.org/10.1049/sej.1994.0025CrossrefGoogle Scholar[12]Maria Christakis and Christian Bird. 2016. What developers want and need from program analysis: an empirical study. In Proceedings of the 31st IEEE/ACM international conference on automated software engineering. 332–343. https://doi.org/10.1145/2970276.2970347Digital LibraryGoogle Scholar[13]Henry Coles, Thomas Laurent, Christopher Henard, Mike Papadakis, and Anthony Ventresque. 2016. Pit: a practical mutation testing tool for java. In Proceedings of the 25th international symposium on software testing and analysis. 449–452. https://doi.org/10.1145/2931037.2948707Digital LibraryGoogle Scholar[14]Mickaël Delahaye and Lydie du Bousquet. 2013. A Comparison of Mutation Analysis Tools for Java. In 2013 13th International Conference on Quality Software. 187–195. https://doi.org/10.1109/QSIC.2013.47Digital LibraryGoogle Scholar[15]Xavier Devroey, Sebastiano Panichella, and Alessio Gambi. 2020. Java unit testing tool competition: Eighth round. In Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops. 545–548. https://doi.org/10.1145/3387940.3392265Digital LibraryGoogle Scholar[16]Elizabeth Dinella, Gabriel Ryan, Shuvendu K. Lahiri, and Todd Mytkowicz. 2022. Replication Artifact for TOGA: A Neural Method for Test Oracle Generation. https://doi.org/10.5281/zenodo.6210589CrossrefGoogle Scholar[17]Elizabeth Dinella, Gabriel Ryan, Todd Mytkowicz, and Shuvendu K. Lahiri. 2022. TOGA: A Neural Method for Test Oracle Generation. In Proceedings of the 44th International Conference on Software Engineering (ICSE ’22). Association for Computing Machinery, New York, NY, USA. 2130–2141. isbn:9781450392211 https://doi.org/10.1145/3510003.3510141Digital LibraryGoogle Scholar[18]Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, and Daxin Jiang. 2020. Codebert: A pre-trained model for programming and natural languages. arXiv preprint arXiv:2002.08155.Google Scholar[19]Gordon Fraser and Andrea Arcuri. 2011. Evosuite: automatic test suite generation for object-oriented software. In Proceedings of the 19th ACM SIGSOFT symposium and the 13th European conference on Foundations of software engineering. 416–419. https://doi.org/10.1145/2025113.2025179Digital LibraryGoogle Scholar[20]Gordon Fraser and Andrea Arcuri. 2014. A large-scale evaluation of automated unit test generation using evosuite. ACM Transactions on Software Engineering and Methodology (TOSEM), 24, 2 (2014), 1–42. https://doi.org/10.1145/2685612Digital LibraryGoogle Scholar[21]Patrice Godefroid, Nils Klarlund, and Koushik Sen. 2005. DART: Directed automated random testing. In Proceedings of the 2005 ACM SIGPLAN conference on Programming language design and implementation. 213–223. https://doi.org/10.1145/1064978.1065036Digital LibraryGoogle Scholar[22]Alberto Goffi, Alessandra Gorla, Michael D. Ernst, and Mauro Pezzè. 2016. Automatic Generation of Oracles for Exceptional Behaviors. In Proceedings of the 25th International Symposium on Software Testing and Analysis (ISSTA 2016). Association for Computing Machinery, New York, NY, USA. 213–224. isbn:9781450343909 https://doi.org/10.1145/2931037.2931061Digital LibraryGoogle Scholar[23]Soneya Binta Hossain. 2023. Artifact: Neural-Based Test Oracle Generation: A Large-scale Evaluation and Lessons Learned. 8, https://doi.org/10.6084/m9.figshare.21973091.v4Digital LibraryGoogle Scholar[24]Soneya Binta Hossain and Matthew B Dwyer. 2022. A Brief Survey on Oracle-based Test Adequacy Metrics. arXiv preprint arXiv:2212.06118, https://doi.org/10.48550/arXiv.2212.06118CrossrefGoogle Scholar[25]Soneya Binta Hossain, Matthew B. Dwyer, Sebastian Elbaum, and Anh Nguyen-Tuong. 2023. Measuring and Mitigating Gaps in Structural Testing. In 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE). 1712–1723. https://doi.org/10.1109/ICSE48619.2023.00147Digital LibraryGoogle Scholar[26]Ali Reza Ibrahimzada, Yigit Varli, Dilara Tekinoglu, and Reyhaneh Jabbarvand. 2022. Perfect is the Enemy of Test Oracle. In Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE 2022). Association for Computing Machinery, 70–81. isbn:9781450394130 https://doi.org/10.1145/3540250.3549086Digital LibraryGoogle Scholar[27]Gunel Jahangirova, David Clark, Mark Harman, and Paolo Tonella. 2016. Test oracle assessment and improvement. In Proceedings of the 25th International Symposium on Software Testing and Analysis. 247–258. https://doi.org/10.1145/2931037.2931062Digital LibraryGoogle Scholar[28]Brittany Johnson, Yoonki Song, Emerson Murphy-Hill, and Robert Bowdidge. 2013. Why don’t software developers use static analysis tools to find bugs? In 2013 35th International Conference on Software Engineering (ICSE). 672–681. https://doi.org/10.1109/ICSE.2013.6606613CrossrefGoogle Scholar[29]René Just, Darioush Jalali, and Michael D Ernst. 2014. Defects4J: A database of existing faults to enable controlled testing studies for Java programs. In Proceedings of the 2014 International Symposium on Software Testing and Analysis. 437–440. https://doi.org/10.1145/2610384.2628055Digital LibraryGoogle Scholar[30]René Just, Darioush Jalali, Laura Inozemtseva, Michael D. Ernst, Reid Holmes, and Gordon Fraser. 2014. Are Mutants a Valid Substitute for Real Faults in Software Testing? In Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering (FSE 2014). Association for Computing Machinery, New York, NY, USA. 654–665. isbn:9781450330565 https://doi.org/10.1145/2635868.2635929Digital LibraryGoogle Scholar[31]Marinos Kintis, Mike Papadakis, Andreas Papadopoulos, Evangelos Valvis, Nicos Malevris, and Yves Le Traon. 2018. How effective are mutation testing tools? An empirical analysis of Java mutation testing tools with manual analysis and real faults. Empirical Software Engineering, 23, 4 (2018), 2426–2463. https://doi.org/10.1007/s10664-017-9582-5Digital LibraryGoogle Scholar[32]Kiran Lakhotia, Phil McMinn, and Mark Harman. 2010. An empirical investigation into branch coverage for C programs using CUTE and AUSTIN. Journal of Systems and Software, 83, 12 (2010), 2379–2391. https://doi.org/10.1016/j.jss.2010.07.026Digital LibraryGoogle Scholar[33]Thomas Laurent, Mike Papadakis, Marinos Kintis, Christopher Henard, Yves Le Traon, and Anthony Ventresque. 2017. Assessing and Improving the Mutation Testing Practice of PIT. In 2017 IEEE International Conference on Software Testing, Verification and Validation (ICST). 430–435. https://doi.org/10.1109/ICST.2017.47CrossrefGoogle Scholar[34]Valentin JM Manès, HyungSeok Han, Choongwoo Han, Sang Kil Cha, Manuel Egele, Edward J Schwartz, and Maverick Woo. 2019. The art, science, and engineering of fuzzing: A survey. IEEE Transactions on Software Engineering, 47, 11 (2019), 2312–2331. https://doi.org/10.1109/TSE.2019.2946563CrossrefGoogle Scholar[35]Ke Mao, Mark Harman, and Yue Jia. 2016. Sapienz: Multi-objective automated testing for android applications. In Proceedings of the 25th international symposium on software testing and analysis. 94–105. https://doi.org/10.1145/2931037.2931054Digital LibraryGoogle Scholar[36]William M McKeeman. 1998. Differential testing for software. Digital Technical Journal, 10, 1 (1998), 100–107.Google Scholar[37]Glenford J Myers, Corey Sandler, and Tom Badgett. 2011. The art of software testing. John Wiley & Sons.Digital LibraryGoogle Scholar[38]Rahul Pandita, Xusheng Xiao, Hao Zhong, Tao Xie, Stephen Oney, and Amit Paradkar. 2012. Inferring method specifications from natural language API descriptions. In 2012 34th International Conference on Software Engineering (ICSE). 815–825. https://doi.org/10.1109/ICSE.2012.6227137CrossrefGoogle Scholar[39]Sebastiano Panichella, Alessio Gambi, Fiorella Zampetti, and Vincenzo Riccio. 2021. Sbst tool competition 2021. In 2021 IEEE/ACM 14th International Workshop on Search-Based Software Testing (SBST). 20–27. https://doi.org/10.1109/SBST52555.2021.00011CrossrefGoogle Scholar[40]Goran Petrović, Marko Ivanković, Gordon Fraser, and René Just. 2021. Does mutation testing improve testing practices? In 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE). 910–921. https://doi.org/10.1109/ICSE43902.2021.00087Digital LibraryGoogle Scholar[41]Goran Petrovic, Marko Ivankovic, Gordon Fraser, and René Just. 2021. Practical mutation testing at scale: A view from Google. IEEE Transactions on Software Engineering, https://doi.org/10.1109/TSE.2021.3107634Digital LibraryGoogle Scholar[42]Goran Petrović, Marko Ivanković, Gordon Fraser, and René Just. 2022. Practical Mutation Testing at Scale: A view from Google. IEEE Transactions on Software Engineering, 48, 10 (2022), 3900–3912. https://doi.org/10.1109/TSE.2021.3107634Digital LibraryGoogle Scholar[43]Apache Commons Proper. 2022. Apache Commons Proper – A repository of reusable Java components. https://commons.apache.org/components.htmlGoogle Scholar[44]Shweta Rani, Bharti Suri, and Sunil Kumar Khatri. 2015. Experimental comparison of automated mutation testing tools for java. In 2015 4th International Conference on Reliability, Infocom Technologies and Optimization (ICRITO) (Trends and Future Directions). 1–6. https://doi.org/10.1109/ICRITO.2015.7359265CrossrefGoogle Scholar[45]Caitlin Sadowski, Edward Aftandilian, Alex Eagle, Liam Miller-Cushon, and Ciera Jaspan. 2018. Lessons from building static analysis tools at google. Commun. ACM, 61, 4 (2018), 58–66. https://doi.org/10.1145/3188720Digital LibraryGoogle Scholar[46]David Schuler and Andreas Zeller. 2013. Checked coverage: an indicator for oracle quality. Software testing, verification and reliability, 23, 7 (2013), 531–551. https://doi.org/10.1109/ICST.2011.32Digital LibraryGoogle Scholar[47]Sergio Segura, Gordon Fraser, Ana B. Sanchez, and Antonio Ruiz-Cortés. 2016. A Survey on Metamorphic Testing. IEEE Transactions on Software Engineering, 42, 9 (2016), 805–824. https://doi.org/10.1109/TSE.2016.2532875CrossrefGoogle Scholar[48]Koushik Sen, Darko Marinov, and Gul Agha. 2005. CUTE: A concolic unit testing engine for C. ACM SIGSOFT Software Engineering Notes, 30, 5 (2005), 263–272. https://doi.org/10.1145/1095430.1081750Digital LibraryGoogle Scholar[49]Sina Shamshiri, René Just, José Miguel Rojas, Gordon Fraser, Phil McMinn, and Andrea Arcuri. 2015. Do Automatically Generated Unit Tests Find Real Faults? An Empirical Study of Effectiveness and Challenges (T). In 2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE). 201–211. https://doi.org/10.1109/ASE.2015.86Digital LibraryGoogle Scholar[50]Kavir Shrestha and Matthew J. Rutherford. 2011. An Empirical Evaluation of Assertions as Oracles. In 2011 Fourth IEEE International Conference on Software Testing, Verification and Validation. 110–119. https://doi.org/10.1109/ICST.2011.50Digital LibraryGoogle Scholar[51]Matt Staats, Michael W Whalen, and Mats PE Heimdahl. 2011. Programs, tests, and oracles: the foundations of testing revisited. In 2011 33rd international conference on software engineering (ICSE). 391–400. https://doi.org/10.1145/1985793.1985847Digital LibraryGoogle Scholar[52]Shin Hwei Tan, Darko Marinov, Lin Tan, and Gary T. Leavens. 2012. @tComment: Testing Javadoc Comments to Detect Comment-Code Inconsistencies. In 2012 IEEE Fifth International Conference on Software Testing, Verification and Validation. 260–269. https://doi.org/10.1109/ICST.2012.106Digital LibraryGoogle Scholar[53]Valerio Terragni, Gunel Jahangirova, Paolo Tonella, and Mauro Pezzè. 2020. Evolutionary improvement of assertion oracles. In Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 1178–1189. https://doi.org/10.1145/3368089.3409758Digital LibraryGoogle Scholar[54]Michele Tufano, Dawn Drain, Alexey Svyatkovskiy, Shao Kun Deng, and Neel Sundaresan. 2020. Unit test case generation with transformers and focal context. arXiv preprint arXiv:2009.05617.Google Scholar[55]Michele Tufano, Dawn Drain, Alexey Svyatkovskiy, and Neel Sundaresan. 2022. Generating accurate assert statements for unit test cases using pretrained transformers. In Proceedings of the 3rd ACM/IEEE International Conference on Automation of Software Test. 54–64. https://doi.org/10.1145/3524481.3527220Digital LibraryGoogle Scholar[56]Tássio Virgínio, Luana Almeida Martins, Larissa Rocha Soares, Railana Santana, Heitor Costa, and Ivan Machado. 2020. An empirical study of automatically-generated tests from the perspective of test smells. In Proceedings of the XXXIV Brazilian Symposium on Software Engineering. 92–96. https://doi.org/10.1145/3422392.3422412Digital LibraryGoogle Scholar[57]Jeffrey M. Voas. 1992. PIE: A dynamic failure-based technique. IEEE Transactions on software Engineering, 18, 8 (1992), 717. https://doi.org/10.1109/32.153381Digital LibraryGoogle Scholar[58]Cody Watson, Michele Tufano, Kevin Moran, Gabriele Bavota, and Denys Poshyvanyk. 2020. On learning meaningful assert statements for unit test cases. In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering. 1398–1409. https://doi.org/10.1145/3377811.3380429Digital LibraryGoogle Scholar[59]Elaine J. Weyuker. 1988. The evaluation of program-based software test data adequacy criteria. Commun. ACM, 31, 6 (1988), 668–675. https://doi.org/10.1145/62959.62963Digital LibraryGoogle Scholar[60]Michael Whalen, Gregory Gay, Dongjiang You, Mats P. E. Heimdahl, and Matt Staats. 2013. Observable modified condition/decision coverage. In 2013 35th International Conference on Software Engineering (ICSE). 102–111. https://doi.org/10.1109/ICSE.2013.6606556CrossrefGoogle Scholar[61]Michal Zalewski. 2017. American fuzzy lop (AFL). URL: http://lcamtuf. coredump. cx/afl.Google Scholar[62]Yucheng Zhang and Ali Mesbah. 2015. Assertions are strongly correlated with test suite effectiveness. In Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering. 214–224. https://doi.org/10.1145/2786805.2786858Digital LibraryGoogle Scholar
Cited ByView allHossain SJiang NZhou QLi XChiang WLyu YNguyen HTripp O(2024)A Deep Dive into Large Language Models for Automated Bug Localization and RepairProceedings of the ACM on Software Engineering10.1145/36607731:FSE(1471-1493)Online publication date: 12-Jul-2024https://dl.acm.org/doi/10.1145/3660773Hossain SRoychoudhury APaiva AAbreu RStorey M(2024)Ensuring Critical Properties of Test Oracles for Effective Bug DetectionProceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings10.1145/3639478.3639791(176-180)Online publication date: 14-Apr-2024https://dl.acm.org/doi/10.1145/3639478.3639791Cotroneo DFoggia AImprota CLiguori PNatella R(2024)Automating the correctness assessment of AI-generated code for security contextsJournal of Systems and Software10.1016/j.jss.2024.112113216(112113)Online publication date: Oct-2024https://doi.org/10.1016/j.jss.2024.112113
Index Terms
Neural-Based Test Oracle Generation: A Large-Scale Evaluation and Lessons LearnedComputing methodologiesMachine learningMachine learning approachesNeural networksSoftware and its engineeringSoftware creation and managementSoftware verification and validationSoftware defect analysisSoftware testing and debugging
Recommendations
Achieving scalable mutation-based generation of whole test suites
Without complete formal specification, automatically generated software tests need to be manually checked in order to detect faults. This makes it desirable to produce the strongest possible test set while keeping the number of tests as small as ...Read MoreArtificial neural networks as multi-networks automated test oracle
One of the important issues in software testing is to provide an automated test oracle. Test oracles are reliable sources of how the software under test must operate. In particular, they are used to evaluate the actual results produced by the software. ...Read MoreStrong mutation-based test data generation using hill climbingSBST '16: Proceedings of the 9th International Workshop on Search-Based Software Testing
Mutation Testing is an effective test criterion for finding faults and assessing the quality of a test suite. Every test criterion requires the generation of test cases, which turns to be a manual and difficult task. In literature, search-based ...Read More
Comments
InformationPublished In
ESEC/FSE 2023: Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software EngineeringNovember 20232215
pagesISBN:9798400703270DOI:10.1145/3611643General Chair: Satish ChandraGoogle, USA,Program Chairs: Kelly BlincoeUniversity of Auckland, New Zealand,Paolo TonellaUSI Lugano, Switzerland
Copyright © 2023 Owner/Author.This work is licensed under a Creative Commons Attribution International 4.0 License.SponsorsSIGSOFT: ACM Special Interest Group on Software EngineeringPublisherAssociation for Computing MachineryNew York, NY, United StatesPublication HistoryPublished: 30 November 2023PermissionsRequest permissions for this article.Request PermissionsCheck for updatesBadgesArtifacts Available / v1.1Author TagsEvoSuiteMutation TestingNeural Test Oracle GenerationTOGAQualifiersResearch-articleFunding SourcesThis material is based in part upon work supported by the DARPA ARCOS program under contract FA8750-20-C-0507, by The Air Force Office of Scientific Research under award number FA9550-21-0164, and by Lockheed Martin Advanced Technology Laboratories.ConferenceESEC/FSE '23Sponsor:SIGSOFTESEC/FSE '23: 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering December 3 - 9, 2023CA, San Francisco, USA
Acceptance RatesOverall Acceptance Rate 112 of 543 submissions, 21%
Contributors
Other MetricsView Article MetricsBibliometrics
Article Metrics
3Total CitationsView Citations805Total DownloadsDownloads (Last 12 months)804Downloads (Last 6 weeks)94Reflects downloads up to 14 Dec 2024
Other MetricsView Author MetricsCitations
Cited ByView allHossain SJiang NZhou QLi XChiang WLyu YNguyen HTripp O(2024)A Deep Dive into Large Language Models for Automated Bug Localization and RepairProceedings of the ACM on Software Engineering10.1145/36607731:FSE(1471-1493)Online publication date: 12-Jul-2024https://dl.acm.org/doi/10.1145/3660773Hossain SRoychoudhury APaiva AAbreu RStorey M(2024)Ensuring Critical Properties of Test Oracles for Effective Bug DetectionProceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings10.1145/3639478.3639791(176-180)Online publication date: 14-Apr-2024https://dl.acm.org/doi/10.1145/3639478.3639791Cotroneo DFoggia AImprota CLiguori PNatella R(2024)Automating the correctness assessment of AI-generated code for security contextsJournal of Systems and Software10.1016/j.jss.2024.112113216(112113)Online publication date: Oct-2024https://doi.org/10.1016/j.jss.2024.112113
View options PDFView or Download as a PDF file.PDF eReaderView online with eReader.eReader
Login optionsCheck if you have access through your login credentials or your institution to get full access on this article.Sign inFull AccessGet this Publication
FiguresOtherShareShare this Publication linkCopy LinkCopied!Copying failed.Share on social mediaXLinkedInRedditFacebookemailAffiliationsSoneya Binta HossainUniversity of Virginia, Charlottesville, USAhttps://orcid.org/0000-0002-7282-061XView ProfileAntonio FilieriAmazon Web Services, Santa Clara, United Stateshttps://orcid.org/0000-0001-9646-646XView ProfileMatthew B. DwyerUniversity of Virginia, Charlottesville, USAhttps://orcid.org/0000-0002-1937-1544View ProfileSebastian ElbaumUniversity of Virginia, Charlottesville, USAhttps://orcid.org/0000-0001-9592-1352View ProfileWillem VisserAmazon Web Services, Santa Clara, United Stateshttps://orcid.org/0000-0002-0913-3091View ProfileDownload PDF
View Table of Contents
Export CitationsSelect Citation formatBibTeXEndNoteACM RefPlease download or close your previous search result export first before starting a new bulk export.Preview is not available.By clicking download,a status dialog will open to start the export process. The process may takea few minutes but once it finishes a file will be downloadable from your browser. You may continue to browse the DL while the export process is in progress.DownloadDownload citationCopy citation
Your Search Results Download Request We are preparing your search results for download ...We will inform you here when the file is ready.Download now!Your Search Results Download RequestYour file of search results citations is now ready.Download now!Your Search Results Download RequestYour search export query has expired. Please try again.

Abstract for Research Paper 3

[2307.16023] Neural-Based Test Oracle Generation: A Large-scale Evaluation and Lessons Learned
Change to arXiv's privacy policy
The arXiv Privacy Policy has changed. By continuing to use arxiv.org, you are agreeing to the privacy policy.
I Understand
Computer Science > Software Engineering
arXiv:2307.16023 (cs)
[Submitted on 29 Jul 2023 (v1), last revised 25 Aug 2023 (this version, v2)]
Title:Neural-Based Test Oracle Generation: A Large-scale Evaluation and Lessons Learned
Authors:Soneya Binta Hossain, Antonio Filieri, Matthew B. Dwyer, Sebastian Elbaum, Willem Visser View a PDF of the paper titled Neural-Based Test Oracle Generation: A Large-scale Evaluation and Lessons Learned, by Soneya Binta Hossain and 4 other authors
View PDF
Abstract:Defining test oracles is crucial and central to test development, but manual construction of oracles is expensive. While recent neural-based automated test oracle generation techniques have shown promise, their real-world effectiveness remains a compelling question requiring further exploration and understanding. This paper investigates the effectiveness of TOGA, a recently developed neural-based method for automatic test oracle generation by Dinella et al. TOGA utilizes EvoSuite-generated test inputs and generates both exception and assertion oracles. In a Defects4j study, TOGA outperformed specification, search, and neural-based techniques, detecting 57 bugs, including 30 unique bugs not detected by other methods. To gain a deeper understanding of its applicability in real-world settings, we conducted a series of external, extended, and conceptual replication studies of TOGA.
In a large-scale study involving 25 real-world Java systems, 223.5K test cases, and 51K injected faults, we evaluate TOGA's ability to improve fault-detection effectiveness relative to the state-of-the-practice and the state-of-the-art. We find that TOGA misclassifies the type of oracle needed 24% of the time and that when it classifies correctly around 62% of the time it is not confident enough to generate any assertion oracle. When it does generate an assertion oracle, more than 47% of them are false positives, and the true positive assertions only increase fault detection by 0.3% relative to prior work. These findings expose limitations of the state-of-the-art neural-based oracle generation technique, provide valuable insights for improvement, and offer lessons for evaluating future automated oracle generation methods.
Subjects:
Software Engineering (cs.SE)
Cite as:
arXiv:2307.16023 [cs.SE]
(or
arXiv:2307.16023v2 [cs.SE] for this version)
https://doi.org/10.48550/arXiv.2307.16023
Focus to learn more
arXiv-issued DOI via DataCite
Related DOI:
https://doi.org/10.1145/3611643.3616265
Focus to learn more
DOI(s) linking to related resources
Submission history From: Soneya Binta Hossain [view email]
[v1]
Sat, 29 Jul 2023 16:34:56 UTC (810 KB)
[v2]
Fri, 25 Aug 2023 22:26:59 UTC (1,200 KB)
Full-text links:
Access Paper:
View a PDF of the paper titled Neural-Based Test Oracle Generation: A Large-scale Evaluation and Lessons Learned, by Soneya Binta Hossain and 4 other authorsView PDFTeX SourceOther Formats
view license
Current browse context: cs.SE
< prev
|
next >
new
|
recent
| 2023-07
Change to browse by:
cs
References & Citations
NASA ADSGoogle Scholar
Semantic Scholar
a
export BibTeX citation
Loading...
BibTeX formatted citation
×
loading...
Data provided by:
Bookmark
Bibliographic Tools
Bibliographic and Citation Tools
Bibliographic Explorer Toggle
Bibliographic Explorer (What is the Explorer?)
Connected Papers Toggle
Connected Papers (What is Connected Papers?)
Litmaps Toggle
Litmaps (What is Litmaps?)
scite.ai Toggle
scite Smart Citations (What are Smart Citations?)
Code, Data, Media
Code, Data and Media Associated with this Article
alphaXiv Toggle
alphaXiv (What is alphaXiv?)
Links to Code Toggle
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub Toggle
DagsHub (What is DagsHub?)
GotitPub Toggle
Gotit.pub (What is GotitPub?)
Huggingface Toggle
Hugging Face (What is Huggingface?)
Links to Code Toggle
Papers with Code (What is Papers with Code?)
ScienceCast Toggle
ScienceCast (What is ScienceCast?)
Demos
Demos
Replicate Toggle
Replicate (What is Replicate?)
Spaces Toggle
Hugging Face Spaces (What is Spaces?)
Spaces Toggle
TXYZ.AI (What is TXYZ.AI?)
Related Papers
Recommenders and Search Tools
Link to Influence Flower
Influence Flower (What are Influence Flowers?)
Core recommender toggle
CORE Recommender (What is CORE?)
Author
Venue
Institution
Topic
About arXivLabs
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.
Which authors of this paper are endorsers? |
Disable MathJax (What is MathJax?)
